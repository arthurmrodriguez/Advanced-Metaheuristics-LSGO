\chapter{Introducción}

Desde sus inicios, la computación evolutiva ha sido capaz de proponer soluciones efectivas a una gran cantidad de problemas de optimización a través de algoritmos bio-inspirados, aquellos que se basan en comportamientos o procesos puramente naturales. Algoritmos Evolutivos (EA, Evolutionary Algorithms), de Nubes de Partículas (PSO, Particle Swarm Optimization), de Evolución Diferencial (DE, Differential Evolution) y de Colonias de Hormigas (ACO, Ant Colony Optimization), responden de manera satisfactoria al ser aplicados a problemas de escalas pequeñas-medianas.

Sin embargo, cuando el tamaño del problema se situa en el rango de los cientos o miles de variables, el espacio de soluciones aumenta de manera exponencial conforme crece el número de éstas, lo que repercute directamente en un aumento significativo de la complejidad del problema. Estas condiciones hacen que para una técnica, anteriormente considerada como efectiva, sea mucho más difícil el encontrar una solución óptima para el problema en cuestión.

Con miras a solventar estas dificultades, surge una nueva vertiente de propuestas dedicadas a la resolución de problemas de optimización de escalas igual o superiores a las mil variables, lo que se conoce como\textit{ Evolutionary Large Scale Global Optimization} o \textbf{ELSGO}\cite{ELSGOI}. Estas nuevas técnicas son, principalmente, producto de los más importantes congresos en computación evolutiva del mundo, como el Congress of Evolutionary Computation (IEEE CEC) o el World Congress on Computational Intelligence (WCCI).

\section{Motivación}

Las propuestas disponibles actualmente muestran resultados satisfactorios al ser evaluadas en función de benchmarks preestablecidos y bien definidos. No obstante, su aplicación real normalmente no trasciende más allá de la propia competición para la que fueron diseñados. Este hecho supone la principal motivación de este trabajo: el estudio de la efectividad y eficiencia de estos algoritmos cuando son aplicados a un problema médico real como lo es, en este caso, \textbf{la optimización de los datos de un electroencefalograma}, EEG por sus siglas en inglés.

Como se ha mencionado anteriormente, la optimización de un electroencefalograma es un problema que no guarda relación con ninguno de los benchmarks conocidos hasta la fecha, por ser este un problema cuya \textbf{complejidad} es \textbf{superior tanto a nivel conceptual como en tiempo y en espacio}. Propuestas que conlleven la resolución de este problema de manera satisfactoria podrían reportar importantes avances en cuanto al diseño e implementación de \textbf{interfaces cerebro-ordenador} (BCI, \textit{brain-computer interfaces}) más potentes, fiables y eficientes.

Este tipo de dispositivos podría ser de ayuda en actividades y tareas del día a día donde la decodificación de un EEG cualitativo es crucial a la hora de reconocer estados cognitivos de orden superior, tales como emociones, memoria o planificación, aspectos que influyen directamente en la toma de decisiones críticas\cite{EvolutionaryBigOpt} en distintos entornos donde es necesaria una interacción y respuesta en tiempo real para garantizar el correcto desempeño de la actividad.

\section{Objetivos}

El objetivo principal de este Trabajo de Fin de Grado consiste en \textbf{realizar un estudio comparativo} de los algoritmos dedicados a resolver problemas de optimización con miles de variables, problemas del tipo \textbf{LSGO}, de forma que se pueda plantear un posible curso de acción o propuesta que sea de utilidad para resolver el problema médico real de la optimización de un electroencefalograma (EEG).

Los siguientes capítulos contendrán el estudio completo en cuestión: desde la definición y representación del problema hasta la selección de posibles técnicas y procedimientos candidatas al estudio, así como del proceso de experimentación, la interpretación y valoración de los resultados obtenidos. Los objetivos principales se encuentran recogidos en los siguientes enunciados:

\begin{itemize}
	\item \textbf{Realizar la descripción y representación del problema del EEG en su totalidad.}
	\item \textbf{Analizar en profundidad los algoritmos y técnicas más prometedoras para la resolución del problema.}
	\item \textbf{Diseñar e implementar un proceso de experimentación riguroso y completo para cumplir los requisitos del estudio.}
	\item \textbf{Evaluar los resultados obtenidos y enunciar la propuesta de solución más adecuada en función de éstos.}
\end{itemize}

Una vez enunciados los objetivos, obtener una perspectiva clara y concisa del problema del EEG es crucial para comprender la magnitud del estudio y todo lo que conlleva, dado que compone la base sobre la que se sustenta éste y es la que marca el curso de acción a tomar en función de los requisitos y necesidades que plantee su resolución. 

\section{Problema: Optimización de los datos de un EEG}\label{Section:QEEG}

\subsection{Panorama actual}

La optimización de los datos de un electroencefalograma, a partir de ahora EEG por sus siglas en inglés, está dentro de la categoría de problemas de Big Data, problemas donde no sólo el tamaño del mismo supone de por si una dificultad, sino que también influyen aspectos como el ruido en los datos, la no existencia de patrones fácilmente reconocibles o restricciones de tiempo inherentes al problema.

En el año 2015 se presenta este problema como candidato a conformar la base del \textbf{Optimization of Big Data Competition, CEC 2015}\cite{EvolutionaryBigOpt} donde se realiza su estudio a través de dos algoritmos multiobjetivo considerados \textit{state-of-the-art}, cuyos resultados, a pesar de considerarse como satisfactorios, demostraron que era necesario disponer de mejores propuestas y métodos que aportasen un rendimiento superior en términos de tiempo y calidad de las soluciones.

En ámbitos médicos como la neurociencia se utilizan dispositivos denominados BCI (Brain-computer Interfaces)\cite{BCI} que se encargan de \textbf{capturar la actividad cerebral} a través de electrodos, con el fin de analizar estos datos, procesarlos y traducirlos en acciones o estados cognitivos que puedan utilizarse para actuar en consecuencia ante una determinada situación.

Los BCIs hacen uso principalmente de los \textbf{electroencefalogramas} (EEG), que son exploraciones neurofisiológicas que registran la actividad bioeléctrica cerebral, concretamente de las neuronas, a través de electrodos (componentes de los BCIs), con el objetivo de detectar o diagnosticar enfermedades o trastornos del sistema nervioso central\cite{EEG}, tales como epilepsia, daños cerebrales de distintos tipos, trastornos psiquiátricos, encefalopatías y demás afecciones\cite{EEG2}. 

Dentro de los EEG existen los denominados \textbf{EEG Cuantitativos}, QEEG\cite{QEEG}, que por medio de una malla de electrodos registran de \textbf{forma simultánea} los impulsos eléctricos de múltiples partes del cerebro. Decodificar de forma efectiva estos QEEG en \textbf{estados cognitivos de orden superior}\cite{EvolutionaryBigOpt}, como pueden ser emociones, recuerdos, estados cerebrales, promovería la creación de BCIs más avanzados que sustenten el uso de estos sistemas tanto en el tratamiento de pacientes con distintos trastornos del sistema nervioso como para las actividades del día a día, sobre todo de aquellas que conlleven la toma de decisiones críticas en tiempo real.

Sin embargo, el correcto funcionamiento de los BCIs se ve normalmente truncado por dos principales aspectos: la cantidad de \textbf{información cerebral \textit{real}} que es captada y la \textbf{distorsión que producen las señales eléctricas no cerebrales}, que quedan plasmadas en el QEEG, empañando los resultados obtenidos y elevando la complejidad del proceso de decodificación. Es aquí donde entran en juego técnicas como el \textbf{Análisis de Componentes Independientes, ICA}, y otras técnicas para \textbf{eliminar la correlación} de los datos reales del QEEG con aquellas interferencias que se denominan \textbf{artifacts}, para ser finalmente separados del QEEG y recomponer el EEG original, acción que al estar actualmente realizada por un ser humano, se vuelve \textbf{totalmente inviable} de cara a los procesos de obtención de QEEG actuales.

La propuesta recogida en\cite{EvolutionaryBigOpt} provee una forma simple de representar el problema de la decodificación de un QEEG, de forma que cualquier técnica, método o algoritmo preparado para el procesamiento de grandes cantidades de datos y la optimización global de miles de variables, pueda ser sometido a pruebas exhaustivas frente a este problema real, y finalmente sugerir, si cabe, un posible candidato que susituya al actual método de decodificación, descrito en el párrafo anterior, con el último fin de dar un salto importante en el diseño de sistemas BCIs para la mejora de la calidad de vida de todas aquellas personas que lo requieran.

Como el estudio se basa completamente en la clara definición del problema en cuestión, los siguientes párrafos contendrán toda la información referente a éste, desde la representación elegida, pasando por las bases de datos disponibles y llegando hasta la definición de la función objetivo que marcará el camino a seguir en este estudio.

\subsection{Representación del problema: QEEG}

Para definir el problema de la optimización de un QEEG se ha empleado una representación que divide al problema en 3 subproblemas con la misma naturaleza que el original, donde únicamente cambian la dimensión del problema y la existencia de ruido en los datos. De esta forma, se ha generado una base de datos sintéticos (generados de forma artificial) que se reunen en tres Datasets \textbf{A, B y C} tal y como se muestra en la siguiente tabla.

\begin{table}[htbp]
	\begin{center}
		\begin{tabular}{| c | c | c | c | c |}
			\hline
			Dataset& Nº Fuentes & Nº Artifacts & Sin ruido & Con ruido \\
			\hline \hline
			A & 4 & 2 & D4 & D4N\\ \hline
			B & 12 & 6 & D12 & D12N \\ \hline
			C & 19 & 6 & D19 & D19N \\ \hline
		\end{tabular}
		\caption{Grupos del problema del EEG.}
		\label{tabla:gruposEEG}
	\end{center}
\end{table}

Cada dataset propone una dificultad distinta en términos del número de fuentes de señales reales y de ruido con una \textbf{varianza de 0.1}. Las fuentes de señales efectivas son la suma del \textbf{número de fuentes de datos reales} y el \textbf{número de artifacts}, donde estos últimos operan en altas frecuencias y amplitudes. 

Las señales de los artifacts \textbf{simulan impulsos electromagnéticos} que se generan de forma involuntaria con movimientos de cualquier parte de nuestro cuerpo durante el tiempo en el que se recogen los datos en el EEG, y al ser señales eléctricas, son captadas por éste mezclándose con las señales reales de nuestro cerebro y empañando el QEGG. Cada señal se muestrea a una frecuencia de \textbf{256Hz,} y los artifacts son activados en intervalos que oscilan entre los últimos \textbf{250ms-500ms} de cada segundo. 

Para el \textbf{Dataset A}, las 6 señales totales se mezclan en 4 señales de datos, $\vv{x_1}, \vv{x_2}, \vv{x_3}$ y $ \vv{x_4} $ compuestas según las siguientes ecuaciones.

\begin{equation}\label{eq:D4}
	\begin{gathered}
		\vv{x_1} = \vv{s_1} + 0.9\vv{s_5}\\
		\vv{x_2} = \vv{s_2} + 0.9\vv{s_6}\\
		\vv{x_3} = \vv{s_3} + \vv{s_5}\\
		\vv{x_4} = \vv{s_4} + \vv{s_6}\\
	\end{gathered}
\end{equation}

Para los \textbf{Datasets B y C}, las 6 señales de artifacts se disparan en las posiciones indicadas  en la figura 1 en\cite{EvolutionaryBigOpt}. De forma análoga, cada fuente de datos real de las 12 (Dataset B) o 19 (Dataset C) se mezcla con cada una de las $k$ fuentes de artifacts siguiendo la ecuacion:

\begin{equation}\label{eq:D12-9}
	\begin{gathered}
	\vv{x_i} = \vv{s_i} + \sum_{k=1}^{N} w_{ik} s_k\\
	w_{ik} = exp (-r^2)
	\end{gathered}
\end{equation}

donde $w_{ik}$ representa el exponencial de la distancia Euclídea al cuadrado con signo negativo de la señal \textit{i-ésima} y el artifact \textit{k-ésimo}. Esta distancia se calcula teniendo en cuenta la posición donde se colocan los electrodos (fuentes de datos reales) y desde donde se disparan los artifacts en una cabeza de radio 0,5dm.

En resumen, un total de \textbf{6 problemas distintos}, \textbf{D4, D4N, D12, D12N, D19 y D19N}, donde cada señal se muestrea a 256Hz, da lugar a \textbf{tres valores de dimensionalidad del problema} que repercuten directamente en la complejidad en cuanto a espacio de soluciones, donde cada dimensión se calcula como el \textbf{número de señales de entrada ($N$)} de cada problema multiplicada por la \textbf{frecuencia de muestreo (256Hz)}. Con esta definición se busca facilitar la representación interna de las soluciones como un único conjunto de valores; así, el \textbf{Dataset A} tiene \textbf{1024} variables, el \textbf{Dataset B} cuenta con \textbf{3072} y el \textbf{Dataset C} concretamente dispone de \textbf{4864} variables.

Esta elección conceptual conforma la base de este estudio a nivel de representación de los datos. Esto permitirá evaluar el grado de desempeño de un algoritmo frente a la optimización de los datos simulados de un EEG, cuyos resultados son extrapolables a una aplicación médica real si se tiene en cuenta el proceso matemático y estadístico sobre el que se sustenta la decodificación de las señales medidas y que se describe a continuación.

\subsection{ICA: Independent Component Analysis}

Un Análisis de Componentes Independientes, ICA en inglés, es un método de procesamiento de señales que permite \textbf{separar en fuentes de datos independientes} aquellas fuentes que han sufrido transformaciones lineales y se encuentran mezcladas, en el caso de un EEG, con distintos artifacts que son finalmente eliminados para obtener la señal original\cite{ICA4Dummies}.

A grandes rasgos un ICA consta de dos pasos, un primer paso donde se elimina toda la correlación de los datos llamado \textit{data whitening} y un segundo paso donde se aplica la matriz de rotación inversa a la transformación aplicada, con el fin de obtener los datos originales.

 Expresado de forma matemática, un ICA busca una \textbf{transformación lineal V} de los \textbf{datos D} de forma tal que $P = V\cdot D$ y que $Cov(P) = I$, siendo $Cov$ la matriz de covarianza e $I$ la matriz de identidad, lo que indica que las variables no tienen correlación alguna (paso 1, \textbf{data whitening}). Tras encontrar $V$, se procede a realizar la rotación de la matriz a través de la \textbf{minimización de la Gaussianidad} de la matriz en cuestión.
 
 Por tanto, el problema de la optimización del QEEG se puede formular análogamente a la aplicación de un ICA de la siguiente manera:
 
 \begin{equation} \label{eq: problem1}
	 \begin{gathered}
		X = A\cdot S + N
	 \end{gathered}
 \end{equation}
 
 donde $X$ es la matriz combinada de señales (\textit{P} en ICA), A es la matriz de combinación que representa la transformación lineal \textit{V} del ICA al combinarse mediante las ecuaciones \ref{eq:D4} o \ref{eq:D12-9}, \textit{N} es el ruido y \textit{S} las fuentes de datos originales; así al disponer de \textit{A} y unas fuentes estimadas $\hat{S}$, hay que encontrar \textit{W} tal que $\hat{S} = W\cdot X$.
 
 \subsection{Formulación del problema: enfoque uniobjetivo}
 
 La formulación original del problema sigue un enfoque multiobjetivo pero adoptar un enfoque uniobjetivo reduce la complejidad del problema sin incurrir en pérdida alguna de generalidad. Por tanto, se utilizará el enfoque multiobjetivo para formular el problema pero experimentalmente se generalizará a uno uniobjetivo.
 
 Sea \textit{X} (matriz combinada de señales) una matriz de dimensión $n\times m$ donde $n$ es el número de \textbf{series de tiempo interdependientes} y $m$ la \textbf{longitud} de cada serie. Sea a su vez \textit{S} (matriz de fuentes reales) una matriz $n\times m$ con $n$ series de tiempo \textbf{independientes} de longitud $m$ y \textit{A} una \textbf{matriz de transformación lineal} $n\times n$. Se sabe que:
 
  \begin{equation}
	 \begin{gathered}
	 	X = A\times S
	 \end{gathered}
 \end{equation}
 
 Por tanto el problema consiste en descomponer S en $S_1$ y $S_2$ tal que $S = S_1 +S_2$ y que además $X = A\times S_1 + A\times S_2$. Y sea $C$ la matriz de \textbf{coeficientes de correlación de Pearson} entre $X$ y $A\times S_1$ definida como:
 
 \begin{equation} \label{eq:covar}
	 \begin{gathered}
	 	C = \frac{covar(X,AS_1)}{\sigma(X)\cdot \sigma (A\cdot S_1)}
	 \end{gathered}
 \end{equation}
 
 donde $covar( )$ representa la matriz de covarianzas y $\sigma$ la varianza.
 El objetivo consiste en \textbf{maximizar los elementos diagonales de C} y conseguir que la \textbf{distancia entre $S$ y $S_1$} sea la mínima posible, lo que a su vez maximice la similitud entre estas. Para ello, se debe encontrar una matriz $S_1$ que minimice el valor de las dos siguientes funciones:
 
  \begin{equation} \label{eq:function1}
	 \begin{gathered}
	 	Minimize \ f_1 = \frac{1}{N^2 -N} \sum_{i}^{} \sum_{j \neq i}^{} C_{ij}^2 + \frac{1}{N} \sum_{i}^{} (1-C_{ij})^2
	 \end{gathered}
 \end{equation}
 
  \begin{equation} \label{eq:function2}
	 \begin{gathered}
	 	Minimize \ f_2 = \frac{1}{N \times M} \sum_{i}^{} \sum_{j}^{} (S_{ij} - S_{1ij})^2
	 \end{gathered}
 \end{equation}
 
 donde $f_1$ representa la cantidad de información real que se ha perdido al reconstruir la matriz de datos reales $S$ y donde $f_2$ representa el porcentaje de artifacts presentes en la señal final. A través de estas dos funciones se propone el  \textbf{enfoque uniobjetivo} donde la función objetivo a optimizar es la siguiente:
 
 \begin{equation} \label{eq:FObj}
	 \begin{gathered}
	 	f = Minimize(f_1 + f_2)
	 \end{gathered}
 \end{equation}
 
 Queda definida por tanto la base sobre la que se construye este estudio, donde la función objetivo juega un papel principal en cuanto al problema de la optimización de los datos de un EEG. En el siguiente capítulo será revisado el estado del arte del problema en cuestión, además del conjunto de problemas más utilizados actualmente para la evaluación de los algoritmos, los denominados benchmarks, y por último los principales algoritmos y técnicas utilizados para problemas de tipo \textbf{LSGO}.